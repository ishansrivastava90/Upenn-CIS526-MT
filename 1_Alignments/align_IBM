#!/usr/bin/env python
import optparse
import sys
from collections import defaultdict
import pickle

optparser = optparse.OptionParser()
optparser.add_option("-d", "--data", dest="train", default="data/hansards", help="Data filename prefix (default=data)")
optparser.add_option("-e", "--english", dest="english", default="e", help="Suffix of English filename (default=e)")
optparser.add_option("-f", "--french", dest="french", default="f", help="Suffix of French filename (default=f)")
optparser.add_option("-t", "--threshold", dest="threshold", default=0.5, type="float", help="Threshold for aligning with Dice's coefficient (default=0.5)")
optparser.add_option("-n", "--num_sentences", dest="num_sents", default=sys.maxint, type="int", help="Number of sentences to use for training and alignment")
(opts, _) = optparser.parse_args()
f_data = "%s.%s" % (opts.train, opts.french)
e_data = "%s.%s" % (opts.train, opts.english)

#sys.stderr.write("Training IBM Model1 ...\n")

# Do Pre-processing
bitext = [[sentence.lower().strip().split() for sentence in pair] for pair in zip(open(f_data), open(e_data))[:opts.num_sents]]


prob_e_f = defaultdict(int)
prev_prob_e_f = defaultdict(int)
f_total = defaultdict(int)
fe_count = defaultdict(int)
norm = defaultdict(int)

#sys.stderr.write("Initializing translation prob. to uniform ...\n")

for (n, (f, e)) in enumerate(bitext):
    for f_i in set(f):
        f_total[f_i] += 1

len_f = len(f_total.keys())
for (n, (f, e)) in enumerate(bitext):

    new_f = ['$null$']
    new_f.extend(f)
    f = new_f

    for f_i in set(f):
        for e_j in set(e):
            prob_e_f[(f_i, e_j)] = 1/float(1+len_f)
            prev_prob_e_f[(f_i, e_j)] = 0

#sys.stderr.write("Running the EM algorithm to find translation prob ...\n")
converged = 0
iteration = 1

null = None
while converged == 0 and iteration <= 15:

#    sys.stderr.write("Running iteration num %i...\n" % iteration)

    for (n, (f, e)) in enumerate(bitext):
        new_f = ['$null$']
        new_f.extend(f)
        f = new_f

        for f_i in set(f):
            f_total[f_i] = 0
            for e_j in set(e):
                fe_count[(f_i, e_j)] = 0


    #Compute Normalization
    for (n, (f, e)) in enumerate(bitext):
        new_f = ['$null$']
        new_f.extend(f)
        f = new_f

        for e_j in set(e):
            norm[e_j] = 0
            for f_i in set(f):
                norm[e_j] += prob_e_f[(f_i, e_j)]

        for e_j in set(e):
            for f_i in set(f):
                fe_count[(f_i, e_j)] += float(prob_e_f[(f_i, e_j)])/norm[e_j]
                f_total[f_i] += float(prob_e_f[(f_i, e_j)])/norm[e_j]

    converged = 1
    f_with_null = ['$null$']
    f_with_null.extend(f_total.keys())

    for f_i in f_with_null:
        for e_j in norm.keys():
            prob_e_f[(f_i, e_j)] = float(fe_count[(f_i, e_j)])/f_total[f_i]

            #Convergence Check
            if converged == 1 and abs(prob_e_f[(f_i, e_j)] - prev_prob_e_f[(f_i, e_j)]) >0.00000001:
                converged = 0
            prev_prob_e_f[(f_i, e_j)] = prob_e_f[(f_i, e_j)]

    iteration += 1

pickle.dump(prob_e_f, open("trans_probabilities/translationProb_IBM_withNull.p","wb"))

# Printing the alignments
for (f, e) in bitext:
    for (i, f_i) in enumerate(f):
        for (j, e_j) in enumerate(e):
            if prob_e_f[(f_i, e_j)] >= opts.threshold:
                sys.stdout.write("%i-%i " % (i,j))
    sys.stdout.write("\n")
