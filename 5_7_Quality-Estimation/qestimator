#!/usr/bin/env python
import optparse
import sys
from collections import defaultdict
import numpy as np
import cv2
import math
import operator
def estimate_quality(source, target, train_feat, test_feat, train_score):

    #Set the weights
    weights = []
    #print len(features[0])
    #for ind in xrange(0,len(features[0])):
    #    print ind
        #weights[ind] = 1

    train_data = np.asarray(train_feat).astype(np.float32)
    train_label = np.asarray(train_score).astype(np.float32)

    #train_data = np.random.randint(0,100,(5,17)).astype(np.float32)   
#    print str(train_data)
    #print len(train_data[0])
    #print "SFSD"
    knn = cv2.KNearest()
    knn.train(train_data,train_label,maxK=1200)

    """
    test_data = np.random.randint(0,100,(1,17)).astype(np.float32)   
    print test_data
    ret, results, neighbours ,dist = knn.find_nearest(test_data, 5)
    print results
    """


    for feat in test_feat[0:]:
    	final_result = {}
        test_data = np.asarray([feat]).astype(np.float32)
        ret, results, neighbours ,dist = knn.find_nearest(test_data,500)
 #       print int(results[0][0])
	#print dist[0]
	#print neighbours[0]
	for ind in xrange(0,len(neighbours[0])):
	    #print neighbours[0][ind] 
	    #print dist[0][ind]
	    if neighbours[0][ind] not in final_result.keys():
		final_result[neighbours[0][ind]] = float(1)/(dist[0][ind]**0.75)
	    else:
		final_result[neighbours[0][ind]] += float(1)/(dist[0][ind]**0.75)
	sorted_res = sorted(final_result.iteritems(), key=operator.itemgetter(1), reverse=True)
	#print final_result
	#print sorted_res
	print int(sorted_res[0][0])
	#print neighbours
	#print dist
     


#
# def get_avg_score(score_list):
#     feature_score = {}
#     for i, index in enumerate(score_list):
#         # sent = ones[index]
#         feature = features[index]
#         total = 0
#         for w, f in enumerate(feature):
#             total += float(f) * weights[w]
#         feature_score[i] = total
#     return sum(feature_score.values()) / float(len(feature_score))
#
# # Extract sentences with corresponding score of 1, 2 or 3
# ones = defaultdict()
# twos = defaultdict()
# threes = defaultdict()
# for i, sent in enumerate(source):
#     if int(score[i]) == 1:
#         ones[i] = sent
#     elif int(score[i]) == 2:
#         twos[i] = sent
#     elif int(score[i]) == 3:
#         threes[i] = sent
#
# ones_avg_score = get_avg_score(ones)
# twos_avg_score = get_avg_score(twos)
# threes_avg_score = get_avg_score(threes)
#
# test_features = [s.strip().split() for s in open("data-test/test_features")]
# total_feature_scores = {}
# for i, feats in enumerate(test_features):
#     total = 0
#     for w, feat in enumerate(feats):
#         total += float(feat) * weights[w]
#     total_feature_scores[i] = total
#
# test_scores = {}
# for i, score in enumerate(total_feature_scores):
#     ones_diff = abs(score - ones_avg_score)
#     twos_diff = abs(score - twos_avg_score)
#     threes_diff = abs(score - threes_avg_score)
#     dummy = [ones_diff, twos_diff, threes_diff]
#     smallest = min(dummy)
#     if smallest == ones_diff:
#         test_scores[i] = "1"
#     elif smallest == twos_diff:
#         test_scores[i] = "2"
#     elif smallest == threes_diff:
#         test_scores[i] = "3"
#
# for score in test_scores.values():
#     sys.stdout.write("%s\n" % score)

if __name__=="__main__":
    optparser = optparse.OptionParser()
    optparser.add_option("-c", "--score", dest="score_train", default="data-train/es-en_score.train", help="Score train set")
    optparser.add_option("-s", "--source", dest="source_train", default="data-train/es-en_source.train", help="Scourse train set")
    optparser.add_option("-t", "--target", dest="target_train", default="data-train/es-en_target.train", help="Target train set")
    optparser.add_option("-f", "--features", dest="feature_train", default="data-train/train_features", help="Feature train set")
    optparser.add_option("-q", "--test_features", dest="feature_test", default="data-test/test_features", help="Feature test set")

    (opts, _) = optparser.parse_args()

    score = [[float(s.strip())] for s in open(opts.score_train).readlines()]
    source = [s.strip() for s in open(opts.source_train).readlines()]
    target = [s.strip().split() for s in open(opts.target_train).readlines()]
    features = [[f for f in s.strip().split()] for s in open(opts.feature_train).readlines()]
    test_features = [[f for f in s.strip().split()] for s in open(opts.feature_test).readlines()]

    #print features[0],len(features[0])
    #print test_features
    estimate_quality(source, target, features, test_features, score)

    #print features[0],len(features[0])
